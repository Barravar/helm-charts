# Global defaults that will apply to all Jobs. Can be overridden at the individual "jobs" level below
global:
  # Default image - can be overriden at the job level under jobs.scripts.image
  image: "registry.barravar.dev.br/cloud-devops/cloud-devops-janitor"
  # -- Default tag - can be overriden under jobs.scripts.tag / defaults to appVersion from Chart.yaml
  tag: "1.1.0"
  # -- Default pullPolicy - can be overriden under jobs.scripts.pullPolicy
  pullPolicy: "Always"
  # Tags to apply to all jobs
  meta:
    team: "cloud-devops"
    businessUnits: "techops"
    costCenter: ""
    environment: "dev"
    slackChannel: "#cloud-devops"
    partOf: "cloud-devops_devops-janitor"
  # Default settings for all jobs. These options are described further down where they can be overridden if needed
  jobDefaults:
    concurrencyPolicy: "Forbid"
    successfulJobsHistoryLimit: 3
    failedJobsHistoryLimit: 5
    backoffLimit: 3
    janitorStorage:
      enabled: false
    janitorVPA:
      enabled: false
    pod:
      restartPolicy: "Never"
      dnsConfig:
        options:
          ndots: 2
  # Allow calling App/Chart to store base64 encoded values to passthrough to scripts
  envValues: ""
  # TODO: Build multi-arch image, nodeSelector and tolerations will have this effect from chart anyway
# List of jobs to be run. Each job should be given a name (required) The name should not contain special characters or space as it will be
# used to reference the directory your job scripts will be in, among other things.
jobs:
  ## Cronjob schedule - cron syntax - https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule-syntax
  - schedule: "@monthly"
    #schedule: "0 0 * * *"  
    # name: janitor-cronjob-name
    # Should we run more than one job at a time
    concurrencyPolicy: "Forbid"
    # How many successful jobs to keep
    successfulJobsHistoryLimit: 3
    # How many failed jobs to keep
    failedJobsHistoryLimit: 5
    # A list of tasks that we want to run on this job. If there are multiple tasks, they will all run sidecar
    tasks:
    ## Each task is given a key name to identify it. If no command or args are given, or entrypoint is not "true", a shell script 
    ## by the name of this key (suffixed by .sh) will be run in this container.
    # your-task-name:
    ## Override global Janitor image for this task. Will still attempt to mount volumes and secrets as set at the job level.
    # image: ""
    ## Override global Image tag
    # tag: ""
    ## Override global pullPolicy for this task
    # pullPolicy: "Always"

    ## You can explicitly set the entrypoint command and args for the task container. This overrides the task key script 
    ## run behaviour above. If neither this nor the name are given for a task, or if entrypoint is "false" the the container 
    ## image:tag ENTRYPOINT will be executed 
    # command:
    #   /bin/sh
    # args:
    # - /bin/sh
    # - -c
    # - scripts/new_script1/init.sh

    ## If true, will execute the container image default ENTRYPOINT command
    # entrypoint: false

    ## If provided, the Janitor task container, when it starts, will attempt to assume this role before running your scripts.
    # iamRole: "arn:aws:iam::123456789012:role/myrole"

    ## These can be set at the task level, overriding what is set at the job level
    # envFrom: {}
    # envVars: {}
    # env: {}
    # resources: {}
    # configMapMounts: {}
    # secretMounts: {}

    # CronJob Labels - Applied at the CronJob, Job & Pod level for each Janitor Job
    labels: {}
    # CronJob Annotations - Applied at the CronJob, Job & Pod level for each Janitor Job
    annotations: {}
    # "iam.amazonaws.com/role": ""

    envFrom: {}
    # env-source-name:
    #   type: secret|configmap (default configmap)
    #   optional: false
    #   prefix: ""
    # another-env-source:
    #   type: "secret" 
    #   prefix: "string-to-start-envs"
    envVars: {}
    #  debugString: DEBUG
    env: {}
    # env-name:
    #   value: "yarp"
    # another-env-name:
    #   type: secret|configmap (default configmap)
    #   key: "key-name"
    #   name: "configmap-name"
    #   optional: false

    # Resource requests applied to all Janitor Tasks, unless they have been overriden in the tasks section
    resources: {}
    #  requests:
    #    cpu: 100m
    #    memory: 128Mi
    #  limits:
    #    memory: 128Mi

    # If specified the pods will be scheduled to only the node that matches the specified labels and values
    nodeSelector:

    # If specified the pods will ignore specific taints on the nodes (not templatable at the moment)
    tolerations:
    # - key: "cron"
    #   effect: "NoSchedule"
    #   operator: "Exists"

    # Create a serviceAccount to use for the job, or use a pre-created one. By default, a unique service account 
    # is created for each Janitor job and this can be customised (e.g. add IRSA annotations)
    serviceAccount:
    #  name: "cloud-devops-devops-janitor-job-name"
    ## If external is true, no serviceAccount will be created, instead the SA of the given .name will be used.
    #  external: false
    #  annotations: {}
    #  labels: {}

    # Create K8s clusterRole or roles and bind them to the Janitor jobs serviceAccount.
    rbac: []
    ## If namespace is set, a K8s role will be created instead of clusterRole
    #  namespace: ""
    ## List of rules for the Janitor SA role/clusterRole (K8s permissions required for your Janitors job)
    #  rules: []

    # If provided, will attempt all tasks to assume this role on startup
    iamRole: "arn:aws:iam::123456789012:role/myrole"
    # Attaches an EFS volume, or JanitorStore, to all containers at the given path. An RWX shared volume hosted 
    # in the Ops account, this can be used as your script requires. The subPath of JanitorStore mounted to
    # your jobs containers will be the name of your job (e.g. /cleanup-ebs-volumes of JanitorStore will be
    # available at /srv/JanitorStorage in the example below.). Only applied to containers using the Janitor image
    janitorStorage:
    # enabled: false
    # mountPath: /srv/JanitorStorage

    # Create a VPA in 'Auto' mode for the Janitor job. Use extraObjects if you want a more customised VPA
    janitorVPA:
    # enabled: false
    # managedResources: ['mem','cpu']
    # containerlist: [ 'all', 'container-name']

    # Create ConfigMaps for this job or others
    configMaps: {}
    # config-map-name:
    #   data:
    #     key1: value1
    #     key2: |
    #       #!/bin/bash
    #       echo "Pointless script right here"

    # Mountpoint configuration for ConfigMaps. When configured at the job level, the specified ConfigMaps will
    # be mounted to all containers 
    configMapMounts: {}
    # configmap-volume-name:
    #   configMapName: name-of-your-configmap
    #   mountPath: /etc/whatever

    # Existing K8s secrets can be mounted into all containers/tasks at the .jobs level, or per container in .jobs[].tasks
    secretMounts: {}
    # secret-volume-name:
    #   secretName: app-secret-name
    #   mountPath: /path/to/secret/dir/

    ## Create ExternalSecret K8s resources. You can then mount the resulting secret with .secretMounts
    externalSecrets: {}
    # external-secret-name:
    #   refreshInterval: 10m
    #   data:
    #     "hello-service/credentials":  # k8s key to use for propery in k8s secret (or .externalsecret.spec.data.remoteRef.secretKey)
    #       key: external-secret/my-app/my-secret1 # ASM secret name
    #       property: password #  ASM secret property
    # another-external-secret:
    #   refreshInterval: 1h
    #   targetName: resulting-secret-name
    #   dataFrom:
    #     "external-secret/my-app/my-secret2":

    ## PersistentVolumeClaimMounts are used to mount a PVC to the container.
    persistentVolumeClaimMounts: {}
    # "pvc-volume-name":
    #   pvcName: whatever-your-pvc-is-called-in-k8s
    #   mountPath: /path/to/pvc/mountpoint
    #   readOnly: false

    # Create PVCs for use with this job. Mount these into the job task containers with .persistentVolumeClaimMounts
    # Two tested use-cases are dynamic EBS volume provisioning, and dynamic EFS volume provisioning (see examples)
    persistentVolumeClaims: {}
    # "pvc-volume-name-to-create":
    #   type: ebs-csi|efs-csi
    #   volumeSize: "10Gi"
    #   accessMode: "ReadWriteOnce"

    # Create EFS PersistentVolume objects, and PVC objects selecting this volume. Note: The AWS EFS volume must 
    # be provisioned elsewhere first. Mount this PVC into the job task containers with .persistentVolumeClaimMounts
    efsVolumes: {}
    # "efs-volume-and-pvc-name"
    #   efsDns: fqdn-to-the-efs-share
    #   accessMode: "ReadWriteOnce"
    #   csiEnable: true
    #   volumeHandle: ""
    #   share: /
    #   path: /

    ## extraObjects is an array of K8s manifests to deploy to support this CronJobs functionality (e.g. custom K8s RBAC)
    ## Not mergeable, YAML in $jobFile takes precedence. TODO: Might be useful as a merge-able dictionary
    extraObjects: []
    # - apiVersion: rbac.authorization.k8s.io/v1
    #   kind: Role
    #   metadata:
    #     namespace: cloud-devops-devops-janitor
    #     name: janitor-kill-nodes
    #   rules:
    #     - apiGroups: [""]
    #       resources: ["nodes"]
    #       verbs: ["list", "delete"]
    # - apiVersion: rbac.authorization.k8s.io/v1
    #   kind: RoleBinding
    #   metadata:
    #     namespace: cloud-devops-devops-janitor
    #     name: janitor-kill-nodes
    #     roleRef:
    #       apiGroup: rbac.authorization.k8s.io
    #       kind: Role
    #       name: janitor-kill-nodes
    #     subjects:
    #     - kind: ServiceAccount
    #       name: cloud-devops-devops-janitor
    #       namespace: cloud-devops-devops-janitor

    vaultIntegration:
      enable: false
      appRoleName: app-something-useful-maybe
      appRoleId: 12345678-1234-ABCD-EF01-123456789A
      tokenPath: "/var/run/secrets/vault"
    ## Deadline (in whole seconds) for starting the Job
    startingDeadlineSeconds: 60
    ## Once a Job reaches activeDeadlineSeconds, all of its running Pods are terminated and the Job status will become type: Failed with reason: DeadlineExceeded.
    activeDeadlineSeconds: 100
    ## How many times to retry the job if it fails
    backoffLimit: 3
    # Pod restart policy & dnsConfig.
    pod:
      # Should we restart the pod if it fails
      restartPolicy: "Never"
      # Ndots patched to the container /etc/resolv.conf
      dnsConfig:
        options:
          ndots: 2
